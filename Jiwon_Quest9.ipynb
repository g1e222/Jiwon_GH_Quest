{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.5890</td>\n",
       "      <td>153.400</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.053730</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.3980</td>\n",
       "      <td>74.080</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.5850</td>\n",
       "      <td>94.030</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.4450</td>\n",
       "      <td>27.230</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.056610</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.4380</td>\n",
       "      <td>94.440</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>0.056880</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>0.3345</td>\n",
       "      <td>0.8902</td>\n",
       "      <td>2.2170</td>\n",
       "      <td>27.190</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.036720</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>3.1800</td>\n",
       "      <td>53.910</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>0.5835</td>\n",
       "      <td>1.3770</td>\n",
       "      <td>3.8560</td>\n",
       "      <td>50.960</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.01486</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>1.0020</td>\n",
       "      <td>2.4060</td>\n",
       "      <td>24.320</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.035020</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>2.0390</td>\n",
       "      <td>23.940</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.072170</td>\n",
       "      <td>0.077430</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>0.3795</td>\n",
       "      <td>1.1870</td>\n",
       "      <td>2.4660</td>\n",
       "      <td>40.510</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.01460</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>3.5640</td>\n",
       "      <td>54.160</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.040610</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>0.9555</td>\n",
       "      <td>3.5680</td>\n",
       "      <td>11.0700</td>\n",
       "      <td>116.200</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.082970</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.04484</td>\n",
       "      <td>0.012840</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>1.0780</td>\n",
       "      <td>2.9030</td>\n",
       "      <td>36.580</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.031260</td>\n",
       "      <td>0.050510</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.02981</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>1.1690</td>\n",
       "      <td>2.0610</td>\n",
       "      <td>19.210</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>0.059360</td>\n",
       "      <td>0.055010</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.01961</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>1.0330</td>\n",
       "      <td>2.8790</td>\n",
       "      <td>32.550</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.01857</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>0.4727</td>\n",
       "      <td>1.2400</td>\n",
       "      <td>3.1950</td>\n",
       "      <td>45.400</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>1.0730</td>\n",
       "      <td>3.8540</td>\n",
       "      <td>54.180</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.031880</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>1.0170</td>\n",
       "      <td>5.8650</td>\n",
       "      <td>112.400</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.018930</td>\n",
       "      <td>0.033910</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.01356</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>2.0580</td>\n",
       "      <td>23.560</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.023870</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.01980</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>0.7477</td>\n",
       "      <td>1.3830</td>\n",
       "      <td>14.670</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.016980</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.2773</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>1.9090</td>\n",
       "      <td>15.700</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.02027</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>0.4388</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>3.3840</td>\n",
       "      <td>44.910</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.053280</td>\n",
       "      <td>0.064460</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.03672</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>0.6917</td>\n",
       "      <td>1.1270</td>\n",
       "      <td>4.3030</td>\n",
       "      <td>93.990</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.012590</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.01083</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>0.8068</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>5.4550</td>\n",
       "      <td>102.600</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.018820</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.01468</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>1.0460</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>7.2760</td>\n",
       "      <td>111.400</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.037990</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>0.02308</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>2.1100</td>\n",
       "      <td>21.050</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.030550</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>0.8529</td>\n",
       "      <td>1.8490</td>\n",
       "      <td>5.6320</td>\n",
       "      <td>93.540</td>\n",
       "      <td>0.010750</td>\n",
       "      <td>0.027220</td>\n",
       "      <td>0.050810</td>\n",
       "      <td>0.019110</td>\n",
       "      <td>0.02293</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>1.0120</td>\n",
       "      <td>3.4980</td>\n",
       "      <td>43.500</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.035760</td>\n",
       "      <td>0.010830</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>0.6003</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>4.6550</td>\n",
       "      <td>61.100</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.030330</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>1.4790</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>11.730</td>\n",
       "      <td>0.015470</td>\n",
       "      <td>0.064570</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.02105</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>0.2784</td>\n",
       "      <td>1.7680</td>\n",
       "      <td>1.6280</td>\n",
       "      <td>20.860</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.041120</td>\n",
       "      <td>0.055530</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>1.0790</td>\n",
       "      <td>2.6150</td>\n",
       "      <td>23.110</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.046530</td>\n",
       "      <td>0.038290</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.02068</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>0.3031</td>\n",
       "      <td>1.3850</td>\n",
       "      <td>2.1770</td>\n",
       "      <td>27.410</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>0.2351</td>\n",
       "      <td>1.5970</td>\n",
       "      <td>1.5390</td>\n",
       "      <td>17.850</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>1.0470</td>\n",
       "      <td>2.0760</td>\n",
       "      <td>23.120</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.021720</td>\n",
       "      <td>0.026150</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>1.3360</td>\n",
       "      <td>2.0660</td>\n",
       "      <td>31.240</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.020990</td>\n",
       "      <td>0.020210</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>12.970</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.01560</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>0.1144</td>\n",
       "      <td>1.0230</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>7.326</td>\n",
       "      <td>0.010270</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>0.02277</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>1.3630</td>\n",
       "      <td>2.0540</td>\n",
       "      <td>18.240</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.02203</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>0.5196</td>\n",
       "      <td>1.9180</td>\n",
       "      <td>3.5640</td>\n",
       "      <td>33.000</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.02466</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>1.3040</td>\n",
       "      <td>2.1150</td>\n",
       "      <td>20.670</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03004</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.4670</td>\n",
       "      <td>1.9940</td>\n",
       "      <td>17.850</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.030510</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.02912</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>1.3670</td>\n",
       "      <td>1.4770</td>\n",
       "      <td>18.760</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>0.01897</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>1.8790</td>\n",
       "      <td>2.1210</td>\n",
       "      <td>17.860</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>0.018340</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.03759</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>0.2116</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.5020</td>\n",
       "      <td>16.830</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>0.038980</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.01695</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>2.2390</td>\n",
       "      <td>1.4370</td>\n",
       "      <td>14.460</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.027360</td>\n",
       "      <td>0.048040</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>0.2441</td>\n",
       "      <td>2.0900</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>16.800</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.02572</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>2.9270</td>\n",
       "      <td>3.6180</td>\n",
       "      <td>29.110</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03004</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>1.1080</td>\n",
       "      <td>2.2240</td>\n",
       "      <td>19.540</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.046390</td>\n",
       "      <td>0.065780</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.01638</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>2.9040</td>\n",
       "      <td>1.9360</td>\n",
       "      <td>16.970</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.029820</td>\n",
       "      <td>0.057380</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.01488</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>1.4920</td>\n",
       "      <td>2.8880</td>\n",
       "      <td>29.840</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.02080</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>3.8960</td>\n",
       "      <td>2.0410</td>\n",
       "      <td>22.810</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01989</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>0.2602</td>\n",
       "      <td>1.2050</td>\n",
       "      <td>2.3620</td>\n",
       "      <td>22.650</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.048440</td>\n",
       "      <td>0.073590</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.02137</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>1.0260</td>\n",
       "      <td>8.7580</td>\n",
       "      <td>118.800</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.078450</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.02057</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>1.2560</td>\n",
       "      <td>7.6730</td>\n",
       "      <td>158.700</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.028910</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.01114</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>2.4630</td>\n",
       "      <td>5.2030</td>\n",
       "      <td>99.040</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.024230</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>3.4250</td>\n",
       "      <td>48.550</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.037310</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>1.5950</td>\n",
       "      <td>5.7720</td>\n",
       "      <td>86.220</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.016640</td>\n",
       "      <td>0.02324</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.4280</td>\n",
       "      <td>2.5480</td>\n",
       "      <td>19.150</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02676</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius mean texture mean perimeter mean area mean smoothness  \\\n",
       "0        17.990        10.38         122.80    1001.0         0.11840   \n",
       "1        20.570        17.77         132.90    1326.0         0.08474   \n",
       "2        19.690        21.25         130.00    1203.0         0.10960   \n",
       "3        11.420        20.38          77.58     386.1         0.14250   \n",
       "4        20.290        14.34         135.10    1297.0         0.10030   \n",
       "5        12.450        15.70          82.57     477.1         0.12780   \n",
       "6        18.250        19.98         119.60    1040.0         0.09463   \n",
       "7        13.710        20.83          90.20     577.9         0.11890   \n",
       "8        13.000        21.82          87.50     519.8         0.12730   \n",
       "9        12.460        24.04          83.97     475.9         0.11860   \n",
       "10       16.020        23.24         102.70     797.8         0.08206   \n",
       "11       15.780        17.89         103.60     781.0         0.09710   \n",
       "12       19.170        24.80         132.40    1123.0         0.09740   \n",
       "13       15.850        23.95         103.70     782.7         0.08401   \n",
       "14       13.730        22.61          93.60     578.3         0.11310   \n",
       "15       14.540        27.54          96.73     658.8         0.11390   \n",
       "16       14.680        20.13          94.74     684.5         0.09867   \n",
       "17       16.130        20.68         108.10     798.8         0.11700   \n",
       "18       19.810        22.15         130.00    1260.0         0.09831   \n",
       "19       13.540        14.36          87.46     566.3         0.09779   \n",
       "20       13.080        15.71          85.63     520.0         0.10750   \n",
       "21        9.504        12.44          60.34     273.9         0.10240   \n",
       "22       15.340        14.26         102.50     704.4         0.10730   \n",
       "23       21.160        23.04         137.20    1404.0         0.09428   \n",
       "24       16.650        21.38         110.00     904.6         0.11210   \n",
       "25       17.140        16.40         116.00     912.7         0.11860   \n",
       "26       14.580        21.53          97.41     644.8         0.10540   \n",
       "27       18.610        20.25         122.10    1094.0         0.09440   \n",
       "28       15.300        25.27         102.40     732.4         0.10820   \n",
       "29       17.570        15.05         115.00     955.1         0.09847   \n",
       "..          ...          ...            ...       ...             ...   \n",
       "539       7.691        25.44          48.34     170.4         0.08668   \n",
       "540      11.540        14.44          74.65     402.9         0.09984   \n",
       "541      14.470        24.99          95.81     656.4         0.08837   \n",
       "542      14.740        25.42          94.70     668.6         0.08275   \n",
       "543      13.210        28.06          84.88     538.4         0.08671   \n",
       "544      13.870        20.70          89.77     584.8         0.09578   \n",
       "545      13.620        23.23          87.19     573.2         0.09246   \n",
       "546      10.320        16.35          65.31     324.9         0.09434   \n",
       "547      10.260        16.58          65.85     320.8         0.08877   \n",
       "548       9.683        19.34          61.05     285.7         0.08491   \n",
       "549      10.820        24.21          68.89     361.6         0.08192   \n",
       "550      10.860        21.48          68.51     360.5         0.07431   \n",
       "551      11.130        22.44          71.49     378.4         0.09566   \n",
       "552      12.770        29.43          81.35     507.9         0.08276   \n",
       "553       9.333        21.94          59.01     264.0         0.09240   \n",
       "554      12.880        28.92          82.50     514.3         0.08123   \n",
       "555      10.290        27.61          65.67     321.4         0.09030   \n",
       "556      10.160        19.59          64.73     311.7         0.10030   \n",
       "557       9.423        27.88          59.26     271.3         0.08123   \n",
       "558      14.590        22.68          96.39     657.1         0.08473   \n",
       "559      11.510        23.93          74.52     403.5         0.09261   \n",
       "560      14.050        27.15          91.38     600.4         0.09929   \n",
       "561      11.200        29.37          70.67     386.0         0.07449   \n",
       "562      15.220        30.62         103.40     716.9         0.10480   \n",
       "563      20.920        25.09         143.00    1347.0         0.10990   \n",
       "564      21.560        22.39         142.00    1479.0         0.11100   \n",
       "565      20.130        28.25         131.20    1261.0         0.09780   \n",
       "566      16.600        28.08         108.30     858.1         0.08455   \n",
       "567      20.600        29.33         140.10    1265.0         0.11780   \n",
       "568       7.760        24.54          47.92     181.0         0.05263   \n",
       "\n",
       "    mean compactness mean concavity mean concave points mean symmetry  \\\n",
       "0            0.27760       0.300100            0.147100        0.2419   \n",
       "1            0.07864       0.086900            0.070170        0.1812   \n",
       "2            0.15990       0.197400            0.127900        0.2069   \n",
       "3            0.28390       0.241400            0.105200        0.2597   \n",
       "4            0.13280       0.198000            0.104300        0.1809   \n",
       "5            0.17000       0.157800            0.080890        0.2087   \n",
       "6            0.10900       0.112700            0.074000        0.1794   \n",
       "7            0.16450       0.093660            0.059850        0.2196   \n",
       "8            0.19320       0.185900            0.093530        0.2350   \n",
       "9            0.23960       0.227300            0.085430        0.2030   \n",
       "10           0.06669       0.032990            0.033230        0.1528   \n",
       "11           0.12920       0.099540            0.066060        0.1842   \n",
       "12           0.24580       0.206500            0.111800        0.2397   \n",
       "13           0.10020       0.099380            0.053640        0.1847   \n",
       "14           0.22930       0.212800            0.080250        0.2069   \n",
       "15           0.15950       0.163900            0.073640        0.2303   \n",
       "16           0.07200       0.073950            0.052590        0.1586   \n",
       "17           0.20220       0.172200            0.102800        0.2164   \n",
       "18           0.10270       0.147900            0.094980        0.1582   \n",
       "19           0.08129       0.066640            0.047810        0.1885   \n",
       "20           0.12700       0.045680            0.031100        0.1967   \n",
       "21           0.06492       0.029560            0.020760        0.1815   \n",
       "22           0.21350       0.207700            0.097560        0.2521   \n",
       "23           0.10220       0.109700            0.086320        0.1769   \n",
       "24           0.14570       0.152500            0.091700        0.1995   \n",
       "25           0.22760       0.222900            0.140100        0.3040   \n",
       "26           0.18680       0.142500            0.087830        0.2252   \n",
       "27           0.10660       0.149000            0.077310        0.1697   \n",
       "28           0.16970       0.168300            0.087510        0.1926   \n",
       "29           0.11570       0.098750            0.079530        0.1739   \n",
       "..               ...            ...                 ...           ...   \n",
       "539          0.11990       0.092520            0.013640        0.2037   \n",
       "540          0.11200       0.067370            0.025940        0.1818   \n",
       "541          0.12300       0.100900            0.038900        0.1872   \n",
       "542          0.07214       0.041050            0.030270        0.1840   \n",
       "543          0.06877       0.029870            0.032750        0.1628   \n",
       "544          0.10180       0.036880            0.023690        0.1620   \n",
       "545          0.06747       0.029740            0.024430        0.1664   \n",
       "546          0.04994       0.010120            0.005495        0.1885   \n",
       "547          0.08066       0.043580            0.024380        0.1669   \n",
       "548          0.05030       0.023370            0.009615        0.1580   \n",
       "549          0.06602       0.015480            0.008160        0.1976   \n",
       "550          0.04227       0.000000            0.000000        0.1661   \n",
       "551          0.08194       0.048240            0.022570        0.2030   \n",
       "552          0.04234       0.019970            0.014990        0.1539   \n",
       "553          0.05605       0.039960            0.012820        0.1692   \n",
       "554          0.05824       0.061950            0.023430        0.1566   \n",
       "555          0.07658       0.059990            0.027380        0.1593   \n",
       "556          0.07504       0.005025            0.011160        0.1791   \n",
       "557          0.04971       0.000000            0.000000        0.1742   \n",
       "558          0.13300       0.102900            0.037360        0.1454   \n",
       "559          0.10210       0.111200            0.041050        0.1388   \n",
       "560          0.11260       0.044620            0.043040        0.1537   \n",
       "561          0.03558       0.000000            0.000000        0.1060   \n",
       "562          0.20870       0.255000            0.094290        0.2128   \n",
       "563          0.22360       0.317400            0.147400        0.2149   \n",
       "564          0.11590       0.243900            0.138900        0.1726   \n",
       "565          0.10340       0.144000            0.097910        0.1752   \n",
       "566          0.10230       0.092510            0.053020        0.1590   \n",
       "567          0.27700       0.351400            0.152000        0.2397   \n",
       "568          0.04362       0.000000            0.000000        0.1587   \n",
       "\n",
       "    mean fractal dimension radius error texture error perimeter error  \\\n",
       "0                  0.07871       1.0950        0.9053          8.5890   \n",
       "1                  0.05667       0.5435        0.7339          3.3980   \n",
       "2                  0.05999       0.7456        0.7869          4.5850   \n",
       "3                  0.09744       0.4956        1.1560          3.4450   \n",
       "4                  0.05883       0.7572        0.7813          5.4380   \n",
       "5                  0.07613       0.3345        0.8902          2.2170   \n",
       "6                  0.05742       0.4467        0.7732          3.1800   \n",
       "7                  0.07451       0.5835        1.3770          3.8560   \n",
       "8                  0.07389       0.3063        1.0020          2.4060   \n",
       "9                  0.08243       0.2976        1.5990          2.0390   \n",
       "10                 0.05697       0.3795        1.1870          2.4660   \n",
       "11                 0.06082       0.5058        0.9849          3.5640   \n",
       "12                 0.07800       0.9555        3.5680         11.0700   \n",
       "13                 0.05338       0.4033        1.0780          2.9030   \n",
       "14                 0.07682       0.2121        1.1690          2.0610   \n",
       "15                 0.07077       0.3700        1.0330          2.8790   \n",
       "16                 0.05922       0.4727        1.2400          3.1950   \n",
       "17                 0.07356       0.5692        1.0730          3.8540   \n",
       "18                 0.05395       0.7582        1.0170          5.8650   \n",
       "19                 0.05766       0.2699        0.7886          2.0580   \n",
       "20                 0.06811       0.1852        0.7477          1.3830   \n",
       "21                 0.06905       0.2773        0.9768          1.9090   \n",
       "22                 0.07032       0.4388        0.7096          3.3840   \n",
       "23                 0.05278       0.6917        1.1270          4.3030   \n",
       "24                 0.06330       0.8068        0.9017          5.4550   \n",
       "25                 0.07413       1.0460        0.9760          7.2760   \n",
       "26                 0.06924       0.2545        0.9832          2.1100   \n",
       "27                 0.05699       0.8529        1.8490          5.6320   \n",
       "28                 0.06540       0.4390        1.0120          3.4980   \n",
       "29                 0.06149       0.6003        0.8225          4.6550   \n",
       "..                     ...          ...           ...             ...   \n",
       "539                0.07751       0.2196        1.4790          1.4450   \n",
       "540                0.06782       0.2784        1.7680          1.6280   \n",
       "541                0.06341       0.2542        1.0790          2.6150   \n",
       "542                0.05680       0.3031        1.3850          2.1770   \n",
       "543                0.05781       0.2351        1.5970          1.5390   \n",
       "544                0.06688       0.2720        1.0470          2.0760   \n",
       "545                0.05801       0.3460        1.3360          2.0660   \n",
       "546                0.06201       0.2104        0.9670          1.3560   \n",
       "547                0.06714       0.1144        1.0230          0.9887   \n",
       "548                0.06235       0.2957        1.3630          2.0540   \n",
       "549                0.06328       0.5196        1.9180          3.5640   \n",
       "550                0.05948       0.3163        1.3040          2.1150   \n",
       "551                0.06552       0.2800        1.4670          1.9940   \n",
       "552                0.05637       0.2409        1.3670          1.4770   \n",
       "553                0.06576       0.3013        1.8790          2.1210   \n",
       "554                0.05708       0.2116        1.3600          1.5020   \n",
       "555                0.06127       0.2199        2.2390          1.4370   \n",
       "556                0.06331       0.2441        2.0900          1.6480   \n",
       "557                0.06059       0.5375        2.9270          3.6180   \n",
       "558                0.06147       0.2254        1.1080          2.2240   \n",
       "559                0.06570       0.2388        2.9040          1.9360   \n",
       "560                0.06171       0.3645        1.4920          2.8880   \n",
       "561                0.05502       0.3141        3.8960          2.0410   \n",
       "562                0.07152       0.2602        1.2050          2.3620   \n",
       "563                0.06879       0.9622        1.0260          8.7580   \n",
       "564                0.05623       1.1760        1.2560          7.6730   \n",
       "565                0.05533       0.7655        2.4630          5.2030   \n",
       "566                0.05648       0.4564        1.0750          3.4250   \n",
       "567                0.07016       0.7260        1.5950          5.7720   \n",
       "568                0.05884       0.3857        1.4280          2.5480   \n",
       "\n",
       "    area error smoothness error compactness error concavity error  \\\n",
       "0      153.400         0.006399          0.049040        0.053730   \n",
       "1       74.080         0.005225          0.013080        0.018600   \n",
       "2       94.030         0.006150          0.040060        0.038320   \n",
       "3       27.230         0.009110          0.074580        0.056610   \n",
       "4       94.440         0.011490          0.024610        0.056880   \n",
       "5       27.190         0.007510          0.033450        0.036720   \n",
       "6       53.910         0.004314          0.013820        0.022540   \n",
       "7       50.960         0.008805          0.030290        0.024880   \n",
       "8       24.320         0.005731          0.035020        0.035530   \n",
       "9       23.940         0.007149          0.072170        0.077430   \n",
       "10      40.510         0.004029          0.009269        0.011010   \n",
       "11      54.160         0.005771          0.040610        0.027910   \n",
       "12     116.200         0.003139          0.082970        0.088900   \n",
       "13      36.580         0.009769          0.031260        0.050510   \n",
       "14      19.210         0.006429          0.059360        0.055010   \n",
       "15      32.550         0.005607          0.042400        0.047410   \n",
       "16      45.400         0.005718          0.011620        0.019980   \n",
       "17      54.180         0.007026          0.025010        0.031880   \n",
       "18     112.400         0.006494          0.018930        0.033910   \n",
       "19      23.560         0.008462          0.014600        0.023870   \n",
       "20      14.670         0.004097          0.018980        0.016980   \n",
       "21      15.700         0.009606          0.014320        0.019850   \n",
       "22      44.910         0.006789          0.053280        0.064460   \n",
       "23      93.990         0.004728          0.012590        0.017150   \n",
       "24     102.600         0.006048          0.018820        0.027410   \n",
       "25     111.400         0.008029          0.037990        0.037320   \n",
       "26      21.050         0.004452          0.030550        0.026810   \n",
       "27      93.540         0.010750          0.027220        0.050810   \n",
       "28      43.500         0.005233          0.030570        0.035760   \n",
       "29      61.100         0.005627          0.030330        0.034070   \n",
       "..         ...              ...               ...             ...   \n",
       "539     11.730         0.015470          0.064570        0.092520   \n",
       "540     20.860         0.012150          0.041120        0.055530   \n",
       "541     23.110         0.007138          0.046530        0.038290   \n",
       "542     27.410         0.004775          0.011720        0.019470   \n",
       "543     17.850         0.004973          0.013720        0.014980   \n",
       "544     23.120         0.006298          0.021720        0.026150   \n",
       "545     31.240         0.005868          0.020990        0.020210   \n",
       "546     12.970         0.007086          0.007247        0.010120   \n",
       "547      7.326         0.010270          0.030840        0.026130   \n",
       "548     18.240         0.007440          0.011230        0.023370   \n",
       "549     33.000         0.008263          0.018700        0.012770   \n",
       "550     20.670         0.009579          0.011040        0.000000   \n",
       "551     17.850         0.003495          0.030510        0.034450   \n",
       "552     18.760         0.008835          0.012330        0.013280   \n",
       "553     17.860         0.010940          0.018340        0.039960   \n",
       "554     16.830         0.008412          0.021530        0.038980   \n",
       "555     14.460         0.012050          0.027360        0.048040   \n",
       "556     16.800         0.012910          0.022220        0.004174   \n",
       "557     29.110         0.011590          0.011240        0.000000   \n",
       "558     19.540         0.004242          0.046390        0.065780   \n",
       "559     16.970         0.008200          0.029820        0.057380   \n",
       "560     29.840         0.007256          0.026780        0.020710   \n",
       "561     22.810         0.007594          0.008878        0.000000   \n",
       "562     22.650         0.004625          0.048440        0.073590   \n",
       "563    118.800         0.006399          0.043100        0.078450   \n",
       "564    158.700         0.010300          0.028910        0.051980   \n",
       "565     99.040         0.005769          0.024230        0.039500   \n",
       "566     48.550         0.005903          0.037310        0.047300   \n",
       "567     86.220         0.006522          0.061580        0.071170   \n",
       "568     19.150         0.007189          0.004660        0.000000   \n",
       "\n",
       "    concave points error symmetry error fractal dimension error worst radius  \\\n",
       "0               0.015870        0.03003                0.006193       25.380   \n",
       "1               0.013400        0.01389                0.003532       24.990   \n",
       "2               0.020580        0.02250                0.004571       23.570   \n",
       "3               0.018670        0.05963                0.009208       14.910   \n",
       "4               0.018850        0.01756                0.005115       22.540   \n",
       "5               0.011370        0.02165                0.005082       15.470   \n",
       "6               0.010390        0.01369                0.002179       22.880   \n",
       "7               0.014480        0.01486                0.005412       17.060   \n",
       "8               0.012260        0.02143                0.003749       15.490   \n",
       "9               0.014320        0.01789                0.010080       15.090   \n",
       "10              0.007591        0.01460                0.003042       19.190   \n",
       "11              0.012820        0.02008                0.004144       20.420   \n",
       "12              0.040900        0.04484                0.012840       20.960   \n",
       "13              0.019920        0.02981                0.003002       16.840   \n",
       "14              0.016280        0.01961                0.008093       15.030   \n",
       "15              0.010900        0.01857                0.005466       17.460   \n",
       "16              0.011090        0.01410                0.002085       19.070   \n",
       "17              0.012970        0.01689                0.004142       20.960   \n",
       "18              0.015210        0.01356                0.001997       27.320   \n",
       "19              0.013150        0.01980                0.002300       15.110   \n",
       "20              0.006490        0.01678                0.002425       14.500   \n",
       "21              0.014210        0.02027                0.002968       10.230   \n",
       "22              0.022520        0.03672                0.004394       18.070   \n",
       "23              0.010380        0.01083                0.001987       29.170   \n",
       "24              0.011300        0.01468                0.002801       26.460   \n",
       "25              0.023970        0.02308                0.007444       22.250   \n",
       "26              0.013520        0.01454                0.003711       17.620   \n",
       "27              0.019110        0.02293                0.004217       21.310   \n",
       "28              0.010830        0.01768                0.002967       20.270   \n",
       "29              0.013540        0.01925                0.003742       20.010   \n",
       "..                   ...            ...                     ...          ...   \n",
       "539             0.013640        0.02105                0.007551        8.678   \n",
       "540             0.014940        0.01840                0.005512       12.260   \n",
       "541             0.011620        0.02068                0.006111       16.220   \n",
       "542             0.012690        0.01870                0.002626       16.510   \n",
       "543             0.009117        0.01724                0.001343       14.370   \n",
       "544             0.009061        0.01490                0.003599       15.050   \n",
       "545             0.009064        0.02087                0.002583       15.350   \n",
       "546             0.005495        0.01560                0.002606       11.250   \n",
       "547             0.010970        0.02277                0.005890       10.830   \n",
       "548             0.009615        0.02203                0.004154       10.930   \n",
       "549             0.005917        0.02466                0.002977       13.030   \n",
       "550             0.000000        0.03004                0.002228       11.660   \n",
       "551             0.010240        0.02912                0.004723       12.020   \n",
       "552             0.009305        0.01897                0.001726       13.870   \n",
       "553             0.012820        0.03759                0.004623        9.845   \n",
       "554             0.007620        0.01695                0.002801       13.890   \n",
       "555             0.017210        0.01843                0.004938       10.840   \n",
       "556             0.007082        0.02572                0.002278       10.650   \n",
       "557             0.000000        0.03004                0.003324       10.490   \n",
       "558             0.016060        0.01638                0.004406       15.480   \n",
       "559             0.012670        0.01488                0.004738       12.480   \n",
       "560             0.016260        0.02080                0.005304       15.300   \n",
       "561             0.000000        0.01989                0.001773       11.920   \n",
       "562             0.016080        0.02137                0.006142       17.520   \n",
       "563             0.026240        0.02057                0.006213       24.290   \n",
       "564             0.024540        0.01114                0.004239       25.450   \n",
       "565             0.016780        0.01898                0.002498       23.690   \n",
       "566             0.015570        0.01318                0.003892       18.980   \n",
       "567             0.016640        0.02324                0.006185       25.740   \n",
       "568             0.000000        0.02676                0.002783        9.456   \n",
       "\n",
       "    worst texture worst perimeter worst area worst smoothness  \\\n",
       "0           17.33          184.60     2019.0          0.16220   \n",
       "1           23.41          158.80     1956.0          0.12380   \n",
       "2           25.53          152.50     1709.0          0.14440   \n",
       "3           26.50           98.87      567.7          0.20980   \n",
       "4           16.67          152.20     1575.0          0.13740   \n",
       "5           23.75          103.40      741.6          0.17910   \n",
       "6           27.66          153.20     1606.0          0.14420   \n",
       "7           28.14          110.60      897.0          0.16540   \n",
       "8           30.73          106.20      739.3          0.17030   \n",
       "9           40.68           97.65      711.4          0.18530   \n",
       "10          33.88          123.80     1150.0          0.11810   \n",
       "11          27.28          136.50     1299.0          0.13960   \n",
       "12          29.94          151.70     1332.0          0.10370   \n",
       "13          27.66          112.00      876.5          0.11310   \n",
       "14          32.01          108.80      697.7          0.16510   \n",
       "15          37.13          124.10      943.2          0.16780   \n",
       "16          30.88          123.40     1138.0          0.14640   \n",
       "17          31.48          136.80     1315.0          0.17890   \n",
       "18          30.88          186.80     2398.0          0.15120   \n",
       "19          19.26           99.70      711.2          0.14400   \n",
       "20          20.49           96.09      630.5          0.13120   \n",
       "21          15.66           65.13      314.9          0.13240   \n",
       "22          19.08          125.10      980.9          0.13900   \n",
       "23          35.59          188.00     2615.0          0.14010   \n",
       "24          31.56          177.00     2215.0          0.18050   \n",
       "25          21.40          152.40     1461.0          0.15450   \n",
       "26          33.21          122.40      896.9          0.15250   \n",
       "27          27.26          139.90     1403.0          0.13380   \n",
       "28          36.71          149.30     1269.0          0.16410   \n",
       "29          19.52          134.90     1227.0          0.12550   \n",
       "..            ...             ...        ...              ...   \n",
       "539         31.89           54.49      223.6          0.15960   \n",
       "540         19.68           78.78      457.8          0.13450   \n",
       "541         31.73          113.50      808.9          0.13400   \n",
       "542         32.29          107.40      826.4          0.10600   \n",
       "543         37.17           92.48      629.6          0.10720   \n",
       "544         24.75           99.17      688.6          0.12640   \n",
       "545         29.09           97.58      729.8          0.12160   \n",
       "546         21.77           71.12      384.9          0.12850   \n",
       "547         22.04           71.08      357.4          0.14610   \n",
       "548         25.59           69.10      364.2          0.11990   \n",
       "549         31.45           83.90      505.6          0.12040   \n",
       "550         24.77           74.08      412.3          0.10010   \n",
       "551         28.26           77.80      436.6          0.10870   \n",
       "552         36.00           88.10      594.7          0.12340   \n",
       "553         25.05           62.86      295.8          0.11030   \n",
       "554         35.74           88.84      595.7          0.12270   \n",
       "555         34.91           69.57      357.6          0.13840   \n",
       "556         22.88           67.88      347.3          0.12650   \n",
       "557         34.24           66.50      330.6          0.10730   \n",
       "558         27.27          105.90      733.5          0.10260   \n",
       "559         37.16           82.28      474.2          0.12980   \n",
       "560         33.17          100.20      706.7          0.12410   \n",
       "561         38.30           75.19      439.6          0.09267   \n",
       "562         42.79          128.70      915.0          0.14170   \n",
       "563         29.41          179.10     1819.0          0.14070   \n",
       "564         26.40          166.10     2027.0          0.14100   \n",
       "565         38.25          155.00     1731.0          0.11660   \n",
       "566         34.12          126.70     1124.0          0.11390   \n",
       "567         39.42          184.60     1821.0          0.16500   \n",
       "568         30.37           59.16      268.6          0.08996   \n",
       "\n",
       "    worst compactness worst concavity worst concave points worst symmetry  \\\n",
       "0             0.66560         0.71190              0.26540         0.4601   \n",
       "1             0.18660         0.24160              0.18600         0.2750   \n",
       "2             0.42450         0.45040              0.24300         0.3613   \n",
       "3             0.86630         0.68690              0.25750         0.6638   \n",
       "4             0.20500         0.40000              0.16250         0.2364   \n",
       "5             0.52490         0.53550              0.17410         0.3985   \n",
       "6             0.25760         0.37840              0.19320         0.3063   \n",
       "7             0.36820         0.26780              0.15560         0.3196   \n",
       "8             0.54010         0.53900              0.20600         0.4378   \n",
       "9             1.05800         1.10500              0.22100         0.4366   \n",
       "10            0.15510         0.14590              0.09975         0.2948   \n",
       "11            0.56090         0.39650              0.18100         0.3792   \n",
       "12            0.39030         0.36390              0.17670         0.3176   \n",
       "13            0.19240         0.23220              0.11190         0.2809   \n",
       "14            0.77250         0.69430              0.22080         0.3596   \n",
       "15            0.65770         0.70260              0.17120         0.4218   \n",
       "16            0.18710         0.29140              0.16090         0.3029   \n",
       "17            0.42330         0.47840              0.20730         0.3706   \n",
       "18            0.31500         0.53720              0.23880         0.2768   \n",
       "19            0.17730         0.23900              0.12880         0.2977   \n",
       "20            0.27760         0.18900              0.07283         0.3184   \n",
       "21            0.11480         0.08867              0.06227         0.2450   \n",
       "22            0.59540         0.63050              0.23930         0.4667   \n",
       "23            0.26000         0.31550              0.20090         0.2822   \n",
       "24            0.35780         0.46950              0.20950         0.3613   \n",
       "25            0.39490         0.38530              0.25500         0.4066   \n",
       "26            0.66430         0.55390              0.27010         0.4264   \n",
       "27            0.21170         0.34460              0.14900         0.2341   \n",
       "28            0.61100         0.63350              0.20240         0.4027   \n",
       "29            0.28120         0.24890              0.14560         0.2756   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.30640         0.33930              0.05000         0.2790   \n",
       "540           0.21180         0.17970              0.06918         0.2329   \n",
       "541           0.42020         0.40400              0.12050         0.3187   \n",
       "542           0.13760         0.16110              0.10950         0.2722   \n",
       "543           0.13810         0.10620              0.07958         0.2473   \n",
       "544           0.20370         0.13770              0.06845         0.2249   \n",
       "545           0.15170         0.10490              0.07174         0.2642   \n",
       "546           0.08842         0.04384              0.02381         0.2681   \n",
       "547           0.22460         0.17830              0.08333         0.2691   \n",
       "548           0.09546         0.09350              0.03846         0.2552   \n",
       "549           0.16330         0.06194              0.03264         0.3059   \n",
       "550           0.07348         0.00000              0.00000         0.2458   \n",
       "551           0.17820         0.15640              0.06413         0.3169   \n",
       "552           0.10640         0.08653              0.06498         0.2407   \n",
       "553           0.08298         0.07993              0.02564         0.2435   \n",
       "554           0.16200         0.24390              0.06493         0.2372   \n",
       "555           0.17100         0.20000              0.09127         0.2226   \n",
       "556           0.12000         0.01005              0.02232         0.2262   \n",
       "557           0.07158         0.00000              0.00000         0.2475   \n",
       "558           0.31710         0.36620              0.11050         0.2258   \n",
       "559           0.25170         0.36300              0.09653         0.2112   \n",
       "560           0.22640         0.13260              0.10480         0.2250   \n",
       "561           0.05494         0.00000              0.00000         0.1566   \n",
       "562           0.79170         1.17000              0.23560         0.4089   \n",
       "563           0.41860         0.65990              0.25420         0.2929   \n",
       "564           0.21130         0.41070              0.22160         0.2060   \n",
       "565           0.19220         0.32150              0.16280         0.2572   \n",
       "566           0.30940         0.34030              0.14180         0.2218   \n",
       "567           0.86810         0.93870              0.26500         0.4087   \n",
       "568           0.06444         0.00000              0.00000         0.2871   \n",
       "\n",
       "    worst fractal dimension  \n",
       "0                   0.11890  \n",
       "1                   0.08902  \n",
       "2                   0.08758  \n",
       "3                   0.17300  \n",
       "4                   0.07678  \n",
       "5                   0.12440  \n",
       "6                   0.08368  \n",
       "7                   0.11510  \n",
       "8                   0.10720  \n",
       "9                   0.20750  \n",
       "10                  0.08452  \n",
       "11                  0.10480  \n",
       "12                  0.10230  \n",
       "13                  0.06287  \n",
       "14                  0.14310  \n",
       "15                  0.13410  \n",
       "16                  0.08216  \n",
       "17                  0.11420  \n",
       "18                  0.07615  \n",
       "19                  0.07259  \n",
       "20                  0.08183  \n",
       "21                  0.07773  \n",
       "22                  0.09946  \n",
       "23                  0.07526  \n",
       "24                  0.09564  \n",
       "25                  0.10590  \n",
       "26                  0.12750  \n",
       "27                  0.07421  \n",
       "28                  0.09876  \n",
       "29                  0.07919  \n",
       "..                      ...  \n",
       "539                 0.10660  \n",
       "540                 0.08134  \n",
       "541                 0.10230  \n",
       "542                 0.06956  \n",
       "543                 0.06443  \n",
       "544                 0.08492  \n",
       "545                 0.06953  \n",
       "546                 0.07399  \n",
       "547                 0.09479  \n",
       "548                 0.07920  \n",
       "549                 0.07626  \n",
       "550                 0.06592  \n",
       "551                 0.08032  \n",
       "552                 0.06484  \n",
       "553                 0.07393  \n",
       "554                 0.07242  \n",
       "555                 0.08283  \n",
       "556                 0.06742  \n",
       "557                 0.06969  \n",
       "558                 0.08004  \n",
       "559                 0.08732  \n",
       "560                 0.08321  \n",
       "561                 0.05905  \n",
       "562                 0.14090  \n",
       "563                 0.09873  \n",
       "564                 0.07115  \n",
       "565                 0.06637  \n",
       "566                 0.07820  \n",
       "567                 0.12400  \n",
       "568                 0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print (cancer.keys())\n",
    "\n",
    "data = pd.DataFrame(cancer.data, columns=[cancer.feature_names])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b574fe29b619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      17.990\n",
      "1      20.570\n",
      "2      19.690\n",
      "3      11.420\n",
      "4      20.290\n",
      "5      12.450\n",
      "6      18.250\n",
      "7      13.710\n",
      "8      13.000\n",
      "9      12.460\n",
      "10     16.020\n",
      "11     15.780\n",
      "12     19.170\n",
      "13     15.850\n",
      "14     13.730\n",
      "15     14.540\n",
      "16     14.680\n",
      "17     16.130\n",
      "18     19.810\n",
      "19     13.540\n",
      "20     13.080\n",
      "21      9.504\n",
      "22     15.340\n",
      "23     21.160\n",
      "24     16.650\n",
      "25     17.140\n",
      "26     14.580\n",
      "27     18.610\n",
      "28     15.300\n",
      "29     17.570\n",
      "        ...  \n",
      "539     7.691\n",
      "540    11.540\n",
      "541    14.470\n",
      "542    14.740\n",
      "543    13.210\n",
      "544    13.870\n",
      "545    13.620\n",
      "546    10.320\n",
      "547    10.260\n",
      "548     9.683\n",
      "549    10.820\n",
      "550    10.860\n",
      "551    11.130\n",
      "552    12.770\n",
      "553     9.333\n",
      "554    12.880\n",
      "555    10.290\n",
      "556    10.160\n",
      "557     9.423\n",
      "558    14.590\n",
      "559    11.510\n",
      "560    14.050\n",
      "561    11.200\n",
      "562    15.220\n",
      "563    20.920\n",
      "564    21.560\n",
      "565    20.130\n",
      "566    16.600\n",
      "567    20.600\n",
      "568     7.760\n",
      "Name: (mean radius,), Length: 569, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "datas=print(data.iloc[:,0])\n",
    "#y = data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "iloc",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'iloc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-055f2f992226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: iloc"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.iloc[:,0]\n",
    "y = data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_score</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.392054</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.878069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.829051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.499850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.274723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_score  y_pred    y\n",
       "0  12.392054     1.0  1.0\n",
       "1   9.878069     1.0  1.0\n",
       "2   9.829051     1.0  1.0\n",
       "3   9.499850     1.0  1.0\n",
       "4   9.274723     1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_score</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-50.015490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>-57.965745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-62.395194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>-68.573861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-110.251841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_score  y_pred    y\n",
       "564  -50.015490     0.0  0.0\n",
       "565  -57.965745     0.0  0.0\n",
       "566  -62.395194     0.0  0.0\n",
       "567  -68.573861     0.0  0.0\n",
       "568 -110.251841     0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model =  LogisticRegression().fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "y_score = model.decision_function(X)  # decision_function():   \n",
    "# f_value(  )  y=1,  y=0 \n",
    "\n",
    "#    \n",
    "df = pd.DataFrame(np.vstack([y_score, y_pred, y]).T,\n",
    "                  columns=[\"y_score\", \"y_pred\", \"y\"])\n",
    "df = df.sort_values(\"y_score\", ascending=False).reset_index(drop=True)\n",
    "display(df.head(), df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[198  14]\n",
      " [  9 348]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9613259668508287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9747899159663865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.968011126564673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95       212\n",
      "           1       0.96      0.97      0.97       357\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       569\n",
      "   macro avg       0.96      0.95      0.96       569\n",
      "weighted avg       0.96      0.96      0.96       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "#  \n",
    "print(confusion_matrix(y, y_pred)) # TP FN FP TN\n",
    "display(accuracy_score(y, y_pred), precision_score(y, y_pred), recall_score(y, y_pred), f1_score(y, y_pred))\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_score</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>-1.651289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-1.662923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-1.798980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>-2.059159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>-2.073938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-2.088739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>-2.097080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>-2.161275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-2.165080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-2.269616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-2.356821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_score  y_hat    y\n",
       "387 -1.651289    1.0  0.0\n",
       "388 -1.662923    1.0  0.0\n",
       "389 -1.798980    1.0  0.0\n",
       "390 -2.059159    0.0  0.0\n",
       "391 -2.073938    0.0  0.0\n",
       "392 -2.088739    0.0  0.0\n",
       "393 -2.097080    0.0  0.0\n",
       "394 -2.161275    0.0  0.0\n",
       "395 -2.165080    0.0  0.0\n",
       "396 -2.269616    0.0  0.0\n",
       "397 -2.356821    0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179  33]\n",
      " [  0 357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.92       212\n",
      "           1       0.92      1.00      0.96       357\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       569\n",
      "   macro avg       0.96      0.92      0.94       569\n",
      "weighted avg       0.95      0.94      0.94       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold 0  -2   \n",
    "lower_threshold = model.decision_function(X) > -2\n",
    "\n",
    "df2 = pd.DataFrame(np.vstack([y_score, lower_threshold, y]).T,\n",
    "                  columns=[\"y_score\", \"y_hat\", \"y\"])\n",
    "df2 = df2.sort_values(\"y_score\", ascending=False).reset_index(drop=True)\n",
    "display(df2[(df2['y_score']> -2.5) & (df2['y_score']<-1.5)])\n",
    "\n",
    "print(confusion_matrix(y, lower_threshold))\n",
    "print(classification_report(y, lower_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "y_score = model.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# decision_function()      y_score  predict_proba()  \n",
    "# probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "# probs = probs[:, 1] # 0,1  1  \n",
    "\n",
    "# calculate AUC\n",
    "roc_auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=2, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate = Recall')\n",
    "plt.ylabel('True Positive Rate = Sensitivity')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# calculate precision-recall AUC\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.plot(recall, precision, lw=2, color='darkorange', label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 ... 2.575e-01 6.638e-01 1.730e-01]\n",
      " ...\n",
      " [1.120e+01 2.937e+01 7.067e+01 ... 0.000e+00 1.566e-01 5.905e-02]\n",
      " [2.092e+01 2.509e+01 1.430e+02 ... 2.542e-01 2.929e-01 9.873e-02]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]] [0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0\n",
      " 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0]\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 ... 2.575e-01 6.638e-01 1.730e-01]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0]\n",
      "[[2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " [2.029e+01 1.434e+01 1.351e+02 ... 1.625e-01 2.364e-01 7.678e-02]\n",
      " ...\n",
      " [2.013e+01 2.825e+01 1.312e+02 ... 1.628e-01 2.572e-01 6.637e-02]\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]] [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
      " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# kFold \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(X[train_index], y[test_index])\n",
    "# cv.split(X) X k-fold    fold (train_index, test_index)  iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95614035, 0.99122807, 0.9122807 , 0.93859649, 0.97345133])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_val_score \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0) # target   shuffle \n",
    "scores = cross_val_score(LogisticRegression(), X, y, cv=kfold)\n",
    "# cross_val_score(, , )\n",
    "scores #  fold  score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93684211, 0.96842105, 0.94179894])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, y) \n",
    "scores \n",
    "# cross_val_score  k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93043478, 0.93913043, 0.97345133, 0.94690265, 0.96460177])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, y, cv=5)\n",
    "scores\n",
    "# cross_val_score     StratifiedKfold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509041939207385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0002539254672343415"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scores.mean(), scores.var()) #     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1','l2']}\n",
    "#     : C, penalty \n",
    "#   'C' 6, 'penalty' 2  =>  6x2=12   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :  {'C': 100, 'penalty': 'l1'}\n",
      "  :  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "  : 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5) #      !\n",
    "# GridSearchCV(,  )\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(' : ', grid_search.best_params_)\n",
    "print('  : ', grid_search.best_estimator_)\n",
    "print('  : {:.2f}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.00855665</td>\n",
       "      <td>0.00359054</td>\n",
       "      <td>0.0155786</td>\n",
       "      <td>0.00529737</td>\n",
       "      <td>0.137902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000812895</td>\n",
       "      <td>0.000479859</td>\n",
       "      <td>0.00228574</td>\n",
       "      <td>0.000894883</td>\n",
       "      <td>0.0602524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.000408268</td>\n",
       "      <td>0.00019083</td>\n",
       "      <td>0.00019908</td>\n",
       "      <td>0.000198364</td>\n",
       "      <td>0.000559711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000481532</td>\n",
       "      <td>0.00038166</td>\n",
       "      <td>0.000398159</td>\n",
       "      <td>0.000396729</td>\n",
       "      <td>0.00047493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_C</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_penalty</th>\n",
       "      <td>l1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l2'}</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.878261</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.895652</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.904348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.93913</td>\n",
       "      <td>0.93913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.929204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.920354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.938053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.913884</td>\n",
       "      <td>0.924429</td>\n",
       "      <td>0.919156</td>\n",
       "      <td>0.927944</td>\n",
       "      <td>0.926186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.0191804</td>\n",
       "      <td>0.0160929</td>\n",
       "      <td>0.0177062</td>\n",
       "      <td>0.0148646</td>\n",
       "      <td>0.0129144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.92511</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.922907</td>\n",
       "      <td>0.942731</td>\n",
       "      <td>0.944934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.914097</td>\n",
       "      <td>0.92511</td>\n",
       "      <td>0.920705</td>\n",
       "      <td>0.936123</td>\n",
       "      <td>0.931718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.927632</td>\n",
       "      <td>0.923246</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.936404</td>\n",
       "      <td>0.940789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.936404</td>\n",
       "      <td>0.934211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.92575</td>\n",
       "      <td>0.920477</td>\n",
       "      <td>0.938052</td>\n",
       "      <td>0.93805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.00429474</td>\n",
       "      <td>0.00283868</td>\n",
       "      <td>0.00316214</td>\n",
       "      <td>0.00250387</td>\n",
       "      <td>0.00469099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0  \\\n",
       "mean_fit_time                          0.00855665   \n",
       "std_fit_time                          0.000812895   \n",
       "mean_score_time                       0.000408268   \n",
       "std_score_time                        0.000481532   \n",
       "param_C                                     0.001   \n",
       "param_penalty                                  l1   \n",
       "params              {'C': 0.001, 'penalty': 'l1'}   \n",
       "split0_test_score                        0.878261   \n",
       "split1_test_score                        0.930435   \n",
       "split2_test_score                        0.929204   \n",
       "split3_test_score                        0.911504   \n",
       "split4_test_score                        0.920354   \n",
       "mean_test_score                          0.913884   \n",
       "std_test_score                          0.0191804   \n",
       "rank_test_score                                12   \n",
       "split0_train_score                        0.92511   \n",
       "split1_train_score                       0.914097   \n",
       "split2_train_score                       0.914474   \n",
       "split3_train_score                       0.914474   \n",
       "split4_train_score                       0.914474   \n",
       "mean_train_score                         0.916526   \n",
       "std_train_score                        0.00429474   \n",
       "\n",
       "                                                1  \\\n",
       "mean_fit_time                          0.00359054   \n",
       "std_fit_time                          0.000479859   \n",
       "mean_score_time                        0.00019083   \n",
       "std_score_time                         0.00038166   \n",
       "param_C                                     0.001   \n",
       "param_penalty                                  l2   \n",
       "params              {'C': 0.001, 'penalty': 'l2'}   \n",
       "split0_test_score                        0.913043   \n",
       "split1_test_score                        0.921739   \n",
       "split2_test_score                        0.911504   \n",
       "split3_test_score                        0.955752   \n",
       "split4_test_score                        0.920354   \n",
       "mean_test_score                          0.924429   \n",
       "std_test_score                          0.0160929   \n",
       "rank_test_score                                10   \n",
       "split0_train_score                       0.929515   \n",
       "split1_train_score                        0.92511   \n",
       "split2_train_score                       0.927632   \n",
       "split3_train_score                       0.921053   \n",
       "split4_train_score                       0.925439   \n",
       "mean_train_score                          0.92575   \n",
       "std_train_score                        0.00283868   \n",
       "\n",
       "                                               2  \\\n",
       "mean_fit_time                          0.0155786   \n",
       "std_fit_time                          0.00228574   \n",
       "mean_score_time                       0.00019908   \n",
       "std_score_time                       0.000398159   \n",
       "param_C                                     0.01   \n",
       "param_penalty                                 l1   \n",
       "params              {'C': 0.01, 'penalty': 'l1'}   \n",
       "split0_test_score                       0.895652   \n",
       "split1_test_score                       0.930435   \n",
       "split2_test_score                       0.911504   \n",
       "split3_test_score                       0.946903   \n",
       "split4_test_score                       0.911504   \n",
       "mean_test_score                         0.919156   \n",
       "std_test_score                         0.0177062   \n",
       "rank_test_score                               11   \n",
       "split0_train_score                      0.922907   \n",
       "split1_train_score                      0.920705   \n",
       "split2_train_score                      0.923246   \n",
       "split3_train_score                      0.914474   \n",
       "split4_train_score                      0.921053   \n",
       "mean_train_score                        0.920477   \n",
       "std_train_score                       0.00316214   \n",
       "\n",
       "                                               3                            4  \n",
       "mean_fit_time                         0.00529737                     0.137902  \n",
       "std_fit_time                         0.000894883                    0.0602524  \n",
       "mean_score_time                      0.000198364                  0.000559711  \n",
       "std_score_time                       0.000396729                   0.00047493  \n",
       "param_C                                     0.01                          0.1  \n",
       "param_penalty                                 l2                           l1  \n",
       "params              {'C': 0.01, 'penalty': 'l2'}  {'C': 0.1, 'penalty': 'l1'}  \n",
       "split0_test_score                       0.904348                     0.904348  \n",
       "split1_test_score                        0.93913                      0.93913  \n",
       "split2_test_score                       0.929204                     0.929204  \n",
       "split3_test_score                       0.920354                     0.920354  \n",
       "split4_test_score                       0.946903                     0.938053  \n",
       "mean_test_score                         0.927944                     0.926186  \n",
       "std_test_score                         0.0148646                    0.0129144  \n",
       "rank_test_score                                8                            9  \n",
       "split0_train_score                      0.942731                     0.944934  \n",
       "split1_train_score                      0.936123                     0.931718  \n",
       "split2_train_score                      0.938596                     0.938596  \n",
       "split3_train_score                      0.936404                     0.940789  \n",
       "split4_train_score                      0.936404                     0.934211  \n",
       "mean_train_score                        0.938052                      0.93805  \n",
       "std_train_score                       0.00250387                   0.00469099  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#     (grid_search.cv_results_)\n",
    "pd.set_option('display.max_columns', None)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "#     \n",
    "display(np.transpose(results.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
